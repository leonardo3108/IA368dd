Building a Simple Information Retrieval System using BM25 and GPT-3 and evaluated in the CISI collection.
--------------------------
Building a simple information retrieval system using BM25 and GPT-3 can be a challenging task, but it is definitely achievable. Here are the steps that you can follow to build the system and evaluate it in the CISI collection:

Obtain the CISI collection: The CISI collection is a standard test collection used for information retrieval research. It consists of approximately 1460 abstracts and associated bibliographic information for articles published in a variety of scientific and technical journals. You can obtain the CISI collection from various sources online.

Preprocess the CISI collection: Before you can use the CISI collection for information retrieval, you need to preprocess it. This involves removing stop words, stemming the words, and converting all the text to lowercase. You can use various natural language processing tools to perform these tasks.

Implement the BM25 algorithm: BM25 is a ranking function used in information retrieval that gives higher scores to documents that contain more relevant terms. You can implement the BM25 algorithm using various programming languages, such as Python. The BM25 algorithm requires three parameters: k1, b, and k3. You can experiment with different values of these parameters to optimize the performance of your system.

Use GPT-3 for query expansion: GPT-3 is a powerful language model that can generate text based on input prompts. You can use GPT-3 to expand the query terms by generating additional relevant terms that can be used to retrieve more relevant documents. You can use various APIs available to access GPT-3, such as the OpenAI API.

Evaluate the system using standard information retrieval metrics: Once you have implemented the BM25 algorithm and used GPT-3 for query expansion, you can evaluate the performance of your system using standard information retrieval metrics, such as precision, recall, and F1 score. You can use the TREC evaluation tools to perform this task.

Overall, building a simple information retrieval system using BM25 and GPT-3 can be a challenging task, but it can be a rewarding experience as well. By following the steps outlined above, you can build a system that can retrieve relevant documents from the CISI collection and achieve good performance in terms of standard information retrieval metrics.




Write a python code to use GPT-3 for query expansion
--------------------------
To use GPT-3 for query expansion in Python, you can use the OpenAI API. Here is an example code that demonstrates how to use the OpenAI API to generate additional relevant terms for a query:

python
Copy code
import openai
import re

### Set up the OpenAI API key
openai.api_key = "YOUR_API_KEY"

### Define the query for which you want to generate additional terms
query = "climate change"

### Define the maximum number of additional terms you want to generate
max_terms = 5

### Define the prompt to be sent to GPT-3
prompt = f"Generate up to {max_terms} additional relevant terms for the query '{query}'."

### Send the prompt to GPT-3 and get the response
response = openai.Completion.create(
  engine="davinci",
  prompt=prompt,
  max_tokens=100,
  n=1,
  stop=None,
  temperature=0.5,
)

### Extract the generated terms from the response
generated_terms = re.findall(r'\n- (.*)', response.choices[0].text)

### Print the generated terms
print(generated_terms)


In the code above, replace YOUR_API_KEY with your actual OpenAI API key. The query variable contains the query for which you want to generate additional terms. The max_terms variable defines the maximum number of additional terms you want to generate. The prompt variable defines the prompt that will be sent to GPT-3, asking it to generate additional terms for the query.

The openai.Completion.create() method is used to send the prompt to GPT-3 and get the response. The engine parameter specifies the GPT-3 engine to be used. The max_tokens parameter specifies the maximum number of tokens to be generated by GPT-3. The n parameter specifies the number of completions to generate. The stop parameter specifies a stop sequence to end the generated text, and the temperature parameter controls the creativity of the generated text.

Finally, the generated terms are extracted from the response using a regular expression and printed to the console. You can then use these additional terms to expand your query and retrieve more relevant documents using the BM25 algorithm.



Write a python code to evaluate a IR system in the CISI collection.
--------------------------
To evaluate an IR system in the CISI collection, you can use standard information retrieval metrics such as precision, recall, and F1 score. Here is an example Python code that demonstrates how to evaluate an IR system using the CISI collection:

python
Copy code
from collections import defaultdict
from math import log10
from typing import List, Tuple

### Load the CISI collection
with open('cisi/cisi.txt') as f:
    cisi_docs = f.read().split('.I ')
cisi_docs = cisi_docs[1:]

### Load the relevance judgments for the CISI collection
with open('cisi/qrels.text') as f:
    cisi_qrels = f.read().splitlines()

### Define the function to preprocess a document or a query
def preprocess(text: str) -> List[str]:
    # Remove punctuation and convert to lowercase
    text = text.lower().translate(str.maketrans('', '', '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'))
    # Split into words
    words = text.split()
    # Remove stop words and stem using Porter stemmer
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words('english'))
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return words

### Define the BM25 algorithm
def bm25(query: str, doc: str, avg_doc_len: float, k1: float, b: float, k3: float, doc_freqs: dict) -> float:
    score = 0
    # Preprocess the query and document
    query_words = preprocess(query)
    doc_words = preprocess(doc)
    # Compute the IDF values for the query terms
    idf_vals = {}
    for word in query_words:
        idf_vals[word] = log10(len(cisi_docs) / doc_freqs[word])
    # Compute the other BM25 parameters for the document
    doc_len = len(doc_words)
    doc_term_freqs = defaultdict(int)
    for word in doc_words:
        doc_term_freqs[word] += 1
    doc_score = 0
    for word in query_words:
        # Compute the TF-IDF score for the query term
        tf = doc_term_freqs[word]
        idf = idf_vals[word]
        tf_idf = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_doc_len))))
        # Compute the BM25 score for the document
        score += tf_idf
        doc_score += ((k3 + 1) * tf * idf) / (k3 + tf)
    score = score * (k3 + 1) * doc_score
    return score

### Define the function to compute the document frequencies of the terms
def compute_doc_freqs(docs: List[str]) -> dict:
    doc_freqs = defaultdict(int)
    for doc in docs:
        words = set(preprocess(doc))
        for word in words:
            doc_freqs[word] += 1
    return doc_freqs

### Compute the average document length and document frequencies of the terms
cisi_doc_freqs = compute_doc_freqs(cisi_docs)
cisi_avg_doc_len = sum(len(preprocess(doc)) for doc in cisi_docs) / len(cisi_docs)

### Define the queries for which to retrieve relevant documents
queries = {
    'q01': 'What problems and concerns are there in making up mixed language bibliographies?',
    'q02': 'What are the authors of articles in the field





Just give me a function to calculate precision, recall and F1 from a BM25 system
--------------------------
Sure, here is a function to calculate precision, recall and F1 from a BM25 system:

python
Copy code
def evaluate_bm25(bm25, queries, qrels, k=10):
    # Initialize counters
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    # Loop over queries and compute precision and recall
    for qid in queries:
        query = queries[qid]
        relevant_docs = set([rel.split()[2] for rel in qrels if rel.startswith(qid)])
        retrieved_docs = set([docid for docid, _ in bm25.get_top_n(query, k)])
        true_positives += len(relevant_docs.intersection(retrieved_docs))
        false_positives += len(retrieved_docs.difference(relevant_docs))
        false_negatives += len(relevant_docs.difference(retrieved_docs))

    # Compute precision, recall and F1
    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)
    f1 = 2 * precision * recall / (precision + recall)

    return precision, recall, f1


The function takes a BM25 system (implemented as a class with a get_top_n method), a dictionary of queries (where each query is a string and the keys are the query IDs), a list of relevance judgments (where each judgment is a string in the format qid 0 docid 1, indicating that document docid is relevant to query qid), and an optional parameter k (the number of documents to retrieve for each query). It returns the precision, recall and F1 scores for the BM25 system on the given queries and relevance judgments.
